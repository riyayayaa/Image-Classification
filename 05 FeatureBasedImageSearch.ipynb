{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LQfRPh0ABLnp"
      },
      "source": [
        "# Deep Learning image search\n",
        "\n",
        "**Image search engines:**\n",
        "Generally speaking, search engine usually takes a query and returns results. Image search engines takes an input image as an image query, then the image search engine finds the \"similar\" images within its indexed database and returns them as the search result.\n",
        "\n",
        "**How to measure similarity between two images?**   \n",
        "- 1- **Pixel space**: One simple way is to measure the euclidean distance between the two images in the pixel space. Accordingly, if two images have common or near values for the corresponding pixels, are then considered \"similar\". This could work some times, however a dolphin and air plane images with blue backgrounds will be considered similar from pixels point of view, and we do not want that!\n",
        "\n",
        "- 2- **Feature space**: Another approach is to use the feature space instead of pixel space when computing the euclidean distance between the two images. In other words, project the images into a space where images with similar features are close to each others. In this space dolphins and airplanes are separated despite pixel level similarity.      \n",
        "\n",
        "**How to get features from images?**  \n",
        "Now the question is, how to project arbitrary image into a space where similar images based on their complex content are grouped together? Well, the answer is easy! Just use a pre-trained generic network such as InceptionV3 trained on the well known ImageNet Large Scale Visual Recognition Challenge (ILSVRC). The network is trained to classify an input image into one of 1000 different classes. Accordingly, if we feed the network with an arbitrary images, and before the output, we can get a strong feature vector that summarizes the content of the input images. that's it.           \n",
        "\n",
        "For the image search engine, we are going to use the feature vector generated by the a pre- trained network (InceptionV3 for instance), instead of the naive pixel wise approach.         \n",
        "\n",
        "**Dataset:**   \n",
        "In the following examples we used the Caltech 101 dataset. For simplicity, only 9 classes are used:\n",
        "\n",
        "`['airplanes', 'Motorbikes', 'Faces', 'Faces_easy', 'Leopards', 'car_side', 'grand_piano', 'brain', 'butterfly']`\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "**Implementation Steps:**   \n",
        "- Download the data and convert it into X (images) and y (labels)  \n",
        "- Randomize the data and select a subset of the data as the dataset on which we want to conduct the search\n",
        "- Load the InceptionV3 network with weights and without the top part to get the high level features.\n",
        "- Compute the feature for all images in the dataset\n",
        "- For a query image: Compute its feature vector. Loop and find the euclidean distance between the query image features and the dataset features. Return the nearest results.       \n",
        "\n",
        "Note: if the dataset is very large, more optimized methods can be used to find the nearest candidates to the query image, such as  KDTree and BallTree.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "euh_3uSO_lAQ"
      },
      "outputs": [],
      "source": [
        "from keras.applications.vgg16 import VGG16\n",
        "from keras.applications.inception_v3 import InceptionV3\n",
        "from keras.preprocessing import image\n",
        "#from keras.applications.vgg16 import preprocess_input\n",
        "from keras.applications.inception_v3 import preprocess_input\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from keras.utils import to_categorical"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pf8Qyer7BLA5"
      },
      "outputs": [],
      "source": [
        "# model = VGG16(weights='imagenet', include_top=False)\n",
        "model = InceptionV3(weights='imagenet', include_top=False)\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m7wDAhzgBLFT"
      },
      "outputs": [],
      "source": [
        "for i, layer in enumerate(model.layers):\n",
        "   print(i, layer.name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "thO1uyUFUwFY"
      },
      "source": [
        "# Caltech 101\n",
        "\n",
        "http://www.vision.caltech.edu/Image_Datasets/Caltech101/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W3ESjq8VUvdl"
      },
      "outputs": [],
      "source": [
        "from urllib.request import urlretrieve\n",
        "import os\n",
        "import tarfile\n",
        "import cv2\n",
        "import pprint\n",
        "import operator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b8REMNVNUuvX"
      },
      "outputs": [],
      "source": [
        "URL_CALTECH_101_DATA = 'http://www.vision.caltech.edu/Image_Datasets/Caltech101/101_ObjectCategories.tar.gz'\n",
        "\n",
        "current_directory = os.path.dirname(os.path.realpath('__file__'))\n",
        "dataset_file_path = current_directory+\"/dataset.tgz\"\n",
        "if os.path.exists(dataset_file_path):\n",
        "    print(\"Already downloaded.\")\n",
        "else:\n",
        "    filename, headers = urlretrieve(URL_CALTECH_101_DATA, dataset_file_path)\n",
        "print(\"Done\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-3GoMrbTUuzS"
      },
      "outputs": [],
      "source": [
        "if (not os.path.exists('./data/')):\n",
        "    os.makedirs('./data/')\n",
        "tar = tarfile.open('./dataset.tgz', \"r:gz\")\n",
        "tar.extractall(path='./data/')\n",
        "tar.close()\n",
        "print(\"Data extracted\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xe9TKSBLUut-"
      },
      "outputs": [],
      "source": [
        "data_directory = \"./data\"\n",
        "categories = os.listdir(data_directory + \"/101_ObjectCategories/\")\n",
        "object_images_count_dict = {}\n",
        "for category in categories:\n",
        "    object_images_count_dict[category] = len(os.listdir(data_directory+\"/101_ObjectCategories/\"+category))\n",
        "object_images_count_dict = sorted(object_images_count_dict.items(), key=operator.itemgetter(1), reverse=True)\n",
        "print(object_images_count_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yoeatxfRUurc"
      },
      "outputs": [],
      "source": [
        "len(object_images_count_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gyesM_2fmtsz"
      },
      "outputs": [],
      "source": [
        "def get_images(object_category, data_directory):\n",
        "    if (not os.path.exists(data_directory)):\n",
        "        print(\"Data not found!\")\n",
        "        return\n",
        "    obj_category_dir = os.path.join(os.path.join(data_directory,\"101_ObjectCategories\"),object_category)\n",
        "    images = [os.path.join(obj_category_dir,img) for img in os.listdir(obj_category_dir)]\n",
        "    return images\n",
        "\n",
        "def read_image(image_path):\n",
        "    img = cv2.imread(image_path, cv2.IMREAD_COLOR)\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) # correct colors\n",
        "    img = cv2.resize(img, (300,200), interpolation=cv2.INTER_CUBIC)\n",
        "    return img"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G5W9HDp2UunF"
      },
      "outputs": [],
      "source": [
        "i = 0\n",
        "X = np.ndarray((10000, 200, 300, 3), dtype=np.uint8)\n",
        "Y = []\n",
        "\n",
        "selected_cls = ['airplanes', 'Motorbikes', 'Faces', 'Faces_easy', 'Leopards', 'car_side', 'grand_piano', 'brain', 'butterfly']\n",
        "\n",
        "for category,_ in object_images_count_dict:\n",
        "    if category in selected_cls:\n",
        "      for image in get_images(category, data_directory):\n",
        "          if not image.endswith('.jpg'):\n",
        "              continue\n",
        "          X[i] = read_image(image)\n",
        "          Y.insert(i,category)\n",
        "          i += 1\n",
        "      print(str(i+1) + \"  \" + category)\n",
        "\n",
        "\n",
        "X = X[:i]\n",
        "print(\"Done\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qPl7u5rmUulJ"
      },
      "outputs": [],
      "source": [
        "X.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SBqK9E5pUuj3"
      },
      "outputs": [],
      "source": [
        "Y[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3-FMdqoNUugV"
      },
      "outputs": [],
      "source": [
        "plt.imshow(X[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "67fOGIICpUKY"
      },
      "outputs": [],
      "source": [
        "label_encoder = LabelEncoder()\n",
        "Y_integer_encoded = label_encoder.fit_transform(Y)\n",
        "Y_integer_encoded"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lQPICsiDpUID"
      },
      "outputs": [],
      "source": [
        "Y_one_hot = to_categorical(Y_integer_encoded)\n",
        "Y_one_hot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oKXfH7OlpUFO"
      },
      "outputs": [],
      "source": [
        "label_encoder.inverse_transform([np.argmax(Y_one_hot[0])])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ChVRcGHKNkKN"
      },
      "outputs": [],
      "source": [
        "# randomize and select sample data\n",
        "x_train, x_test, y_train, y_test = train_test_split(X, Y_one_hot, test_size=0.3, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aLuHPw_RNkI3"
      },
      "outputs": [],
      "source": [
        "fig=plt.figure(figsize=(20, 15))\n",
        "columns = 10\n",
        "rows = 10\n",
        "for i in range(0, columns*rows):\n",
        "    ax = fig.add_subplot(rows, columns, i+1)\n",
        "    plt.imshow(x_test[i])\n",
        "    t = label_encoder.inverse_transform([np.argmax(y_test[i])])[0]\n",
        "    plt.title(str(i) +\":\"+ t)\n",
        "    ax.get_xaxis().set_visible(False)\n",
        "    ax.get_yaxis().set_visible(False)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lp5g3k-PB9Et"
      },
      "outputs": [],
      "source": [
        "for i in range(len(selected_cls)):\n",
        "  print(i , label_encoder.inverse_transform([i])[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BAlmIZwcy46y"
      },
      "outputs": [],
      "source": [
        "data_set = preprocess_input(x_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MpZJnR1Fq1Cq"
      },
      "outputs": [],
      "source": [
        "feats = model.predict(data_set)\n",
        "feats = np.squeeze(feats)\n",
        "print(feats.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7kG7ejMHsxUN"
      },
      "outputs": [],
      "source": [
        "Q_id = 10\n",
        "Query_image = x_test[Q_id]\n",
        "plt.imshow(Query_image)\n",
        "Query_image = preprocess_input(Query_image)\n",
        "Query_feats = model.predict(np.expand_dims(Query_image, axis=0))\n",
        "Query_feats = Query_feats.squeeze()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1FsnYz-ls0BP"
      },
      "outputs": [],
      "source": [
        "# Euclidean distance\n",
        "results = []\n",
        "for i in range(feats.shape[0]):\n",
        "  d = np.linalg.norm(feats[i].flatten() - Query_feats.flatten())\n",
        "  results.append((d, i))\n",
        "\n",
        "results = sorted(results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0vnbk7Mtsz-6"
      },
      "outputs": [],
      "source": [
        "fig=plt.figure(figsize=(10, 8))\n",
        "columns = 4\n",
        "rows = 4\n",
        "for i in range(0, columns*rows):\n",
        "    ax = fig.add_subplot(rows, columns, i+1)\n",
        "    plt.imshow(x_test[results[i][1]])\n",
        "    ax.get_xaxis().set_visible(False)\n",
        "    ax.get_yaxis().set_visible(False)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k_H3YN-bsz5-"
      },
      "outputs": [],
      "source": [
        "# Euclidean distance\n",
        "results = []\n",
        "for i in range(x_test.shape[0]):\n",
        "  d = np.linalg.norm(x_test[i].flatten() - x_test[Q_id].flatten())\n",
        "  results.append((d, i))\n",
        "\n",
        "results = sorted(results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gbM1YB5BERIk"
      },
      "outputs": [],
      "source": [
        "fig=plt.figure(figsize=(10, 8))\n",
        "columns = 4\n",
        "rows = 4\n",
        "for i in range(0, columns*rows):\n",
        "    ax = fig.add_subplot(rows, columns, i+1)\n",
        "    plt.imshow(x_test[results[i][1]])\n",
        "    ax.get_xaxis().set_visible(False)\n",
        "    ax.get_yaxis().set_visible(False)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vUczEsBZLkL-"
      },
      "outputs": [],
      "source": [
        "# http://alexanderfabisch.github.io/t-sne-in-scikit-learn.html\n",
        "def plot_dataset(X, y, X_embedded, min_dist=10.0):\n",
        "    fig = plt.figure(figsize=(15, 15))\n",
        "    ax = plt.axes(frameon=False)\n",
        "    plt.subplots_adjust(left=0.0, bottom=0.0, right=1.0, top=0.9, wspace=0.0, hspace=0.0)\n",
        "\n",
        "    plt.scatter(X_embedded[:, 0], X_embedded[:, 1], c=y)\n",
        "\n",
        "    if min_dist is not None:\n",
        "        from matplotlib import offsetbox\n",
        "        shown_images = np.array([[15., 15.]])\n",
        "        indices = np.arange(X_embedded.shape[0])\n",
        "        np.random.shuffle(indices)\n",
        "        for i in indices[:5000]:\n",
        "            dist = np.sum((X_embedded[i] - shown_images) ** 2, 1)\n",
        "            if np.min(dist) < min_dist:\n",
        "                continue\n",
        "            shown_images = np.r_[shown_images, [X_embedded[i]]]\n",
        "            res = cv2.resize(X[i], dsize=(48, 48), interpolation=cv2.INTER_CUBIC)\n",
        "            imagebox = offsetbox.AnnotationBbox(offsetbox.OffsetImage(res), X_embedded[i])\n",
        "            ax.add_artist(imagebox)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lvA1KXmoHp5b"
      },
      "outputs": [],
      "source": [
        "n_tsne = 300"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WkBzKVvIHp3U"
      },
      "outputs": [],
      "source": [
        "tsne_feats = np.reshape(feats, (feats.shape[0], feats.shape[1]*feats.shape[2]*feats.shape[3]))[:n_tsne]\n",
        "tsne_f = TSNE(n_components=2, perplexity=30, verbose=2).fit_transform(tsne_feats)\n",
        "print(tsne_f.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-xxKhi0VHp1Q"
      },
      "outputs": [],
      "source": [
        "c = np.argmax(y_test[:n_tsne], axis=1)\n",
        "\n",
        "plt.figure(figsize=(7, 7))\n",
        "plt.scatter(tsne_f[:, 0], tsne_f[:, 1], c=c)\n",
        "\n",
        "plt.colorbar()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pmoEPjGMHpyo"
      },
      "outputs": [],
      "source": [
        "plot_dataset(x_test[:n_tsne], c[:n_tsne], tsne_f[:n_tsne], min_dist=1.0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pj1F8zYqEQ_d"
      },
      "outputs": [],
      "source": [
        "tsne_pix = np.reshape(x_test, (x_test.shape[0], x_test.shape[1]*x_test.shape[2]*x_test.shape[3]))[:n_tsne]\n",
        "tsne_p = TSNE(n_components=2, perplexity=30, verbose=2).fit_transform(tsne_pix)\n",
        "print(tsne_p.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "63FmKnDlHGRa"
      },
      "outputs": [],
      "source": [
        "c = np.argmax(y_test[:n_tsne], axis=1)\n",
        "\n",
        "plt.figure(figsize=(7, 7))\n",
        "plt.scatter(tsne_p[:, 0], tsne_p[:, 1], c=c)\n",
        "\n",
        "plt.colorbar()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IUvaCJTKHGOg"
      },
      "outputs": [],
      "source": [
        "plot_dataset(x_test[:n_tsne], c[:n_tsne], tsne_p[:n_tsne], min_dist=1.0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5LqMHEedLxMr"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "05 FeatureBasedImageSearch.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}